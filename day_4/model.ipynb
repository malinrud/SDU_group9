{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.9376\n",
      "Epoch [2/5], Loss: 0.8663\n",
      "Epoch [3/5], Loss: 0.2909\n",
      "Epoch [4/5], Loss: 0.4442\n",
      "Epoch [5/5], Loss: 0.4665\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Add L1 regularization to the 2 nd layer (the layer after the input layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter that controls the strength of the L1 regularization\n",
    "def l1_regularization(model, lambda_l1=0.001):\n",
    "    l1_loss = 0\n",
    "    # Access the fc2 layer's parameters\n",
    "    for param in model.fc2.parameters():\n",
    "        # computes the L1 norm of the fc2 layer’s parameters\n",
    "        l1_loss += torch.sum(torch.abs(param))\n",
    "    return lambda_l1 * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6510\n",
      "Epoch [2/5], Loss: 0.6608\n",
      "Epoch [3/5], Loss: 0.6541\n",
      "Epoch [4/5], Loss: 0.5606\n",
      "Epoch [5/5], Loss: 0.5737\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Add L1 regularization loss\n",
    "        l1_loss = l1_regularization(model)\n",
    "        total_loss = loss + l1_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(total_loss.item())\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Add L2 regularization instead on the 2 nd layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter that controls the strength of the L2 regularization\n",
    "def l2_regularization(model, lambda_l2=0.001):\n",
    "    l2_loss = 0\n",
    "    # Access the fc2 layer's parameters\n",
    "    for param in model.fc2.parameters():\n",
    "        # computes the L2 norm (squared) of the fc2 layer’s parameters\n",
    "        l2_loss += torch.sum(param ** 2)\n",
    "    return lambda_l2 * l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1078\n",
      "Epoch [2/5], Loss: 0.2450\n",
      "Epoch [3/5], Loss: 0.3691\n",
      "Epoch [4/5], Loss: 0.2110\n",
      "Epoch [5/5], Loss: 0.1722\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Add L2 regularization loss\n",
    "        l2_loss = l2_regularization(model)\n",
    "        total_loss = loss + l2_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(total_loss.item())\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What do you observe? (Hint: The lambda value used has a big impact on\n",
    "performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization generally results in lower and more stable training losses compared to L1 regularization, which can cause higher initial losses and less consistent improvement. L2 is more effective at reducing overfitting and improving performance on the training data, while L1 tends to induce sparsity but may not perform as well in this context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity refers to the presence of many zero or near-zero values in the weight matrix. L1 regularization encourages sparsity by penalizing the absolute values of weights, often leading to some weights being exactly zero. This can result in a more compact model where only a subset of features is used. L2 regularization, on the other hand, penalizes the squared values of weights and typically results in smaller weights overall but does not force them to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the purpose of adding regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of adding regularization is to prevent overfitting by penalizing large weights or complex models, thereby improving the model's ability to generalize to new, unseen data. Regularization helps to balance the trade-off between fitting the training data and maintaining model simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dropout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Add a dropout layer between the first and second layer. What do you\n",
    "observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetworkWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with a 50% dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWithDropout = MyNetworkWithDropout()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelWithDropout.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.9063\n",
      "Epoch [2/5], Loss: 0.6110\n",
      "Epoch [3/5], Loss: 0.5800\n",
      "Epoch [4/5], Loss: 0.3818\n",
      "Epoch [5/5], Loss: 0.2768\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelWithDropout.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelWithDropout(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases more smoothly and consistently over epochs with dropout compared to previous runs without it, indicating improved training stability and potentially better generalization. Dropout helps to prevent overfitting by regularizing the model, leading to more gradual and reliable reductions in loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the purpose of adding dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of adding dropout is to prevent overfitting by randomly deactivating a fraction of neurons during training, which forces the network to learn more robust and generalized features. This regularization technique helps improve the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Experiment with different amount of layers. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding More Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDeep = DeepNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelDeep.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.2772\n",
      "Epoch [2/5], Loss: 1.9878\n",
      "Epoch [3/5], Loss: 0.7202\n",
      "Epoch [4/5], Loss: 0.2473\n",
      "Epoch [5/5], Loss: 0.2541\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelDeep.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelDeep(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional layers in the deep model result in a more variable loss pattern compared to the regular model, with larger fluctuations in loss between epochs, indicating potential overfitting or instability in learning. However, the final loss is lower, suggesting that the increased model capacity may capture more complex features, improving performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelShallow = ShallowNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelShallow.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7293\n",
      "Epoch [2/5], Loss: 0.6829\n",
      "Epoch [3/5], Loss: 0.2837\n",
      "Epoch [4/5], Loss: 0.1445\n",
      "Epoch [5/5], Loss: 0.3405\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelShallow.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelShallow(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shallow model shows a more stable loss pattern with a final loss that is generally higher compared to both the regular and deeper models. This suggests that the reduced capacity of the shallow model may lead to underfitting, as it struggles to capture complex patterns in the data, resulting in higher training losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with different amount of neurons in each layer. What do you\n",
    "observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the Number of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WideNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWide = WideNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelWide.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6400\n",
      "Epoch [2/5], Loss: 0.4006\n",
      "Epoch [3/5], Loss: 0.4633\n",
      "Epoch [4/5], Loss: 0.1802\n",
      "Epoch [5/5], Loss: 0.2223\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelWide.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelWide(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wide model shows more variability in the loss values across epochs compared to the normal model, with higher final loss, indicating potential overfitting or instability in learning. While the wide model has increased capacity, it may not necessarily improve performance on the given task and could require additional regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the Number of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrowNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NarrowNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNarrow = NarrowNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modelNarrow.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3166\n",
      "Epoch [2/5], Loss: 0.3643\n",
      "Epoch [3/5], Loss: 0.3100\n",
      "Epoch [4/5], Loss: 0.4129\n",
      "Epoch [5/5], Loss: 0.4244\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    modelNarrow.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelNarrow(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The narrow model has higher initial and overall loss compared to the normal model, indicating that it may struggle with learning complex patterns due to its limited capacity. Despite a significant drop in loss during training, the final loss values suggest it might still be underfitting and unable to fully capture the data's underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Momentum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Try to add momentum to the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNetwork()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1242\n",
      "Epoch [2/5], Loss: 0.1112\n",
      "Epoch [3/5], Loss: 0.3567\n",
      "Epoch [4/5], Loss: 0.0833\n",
      "Epoch [5/5], Loss: 0.0528\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with momentum demonstrates significantly lower loss values and greater stability across epochs compared to the regular model, indicating faster convergence and potentially better training performance. Momentum helps the optimizer to navigate the loss landscape more effectively, reducing fluctuations and achieving a better final loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Test different values of momentum. What value do you get the highest\n",
    "accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate the model\n",
    "def train_and_evaluate(momentum_value):\n",
    "    model = MyNetwork()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=momentum_value)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum: 0.0, Accuracy: 91.44%\n",
      "Momentum: 0.2, Accuracy: 91.58%\n",
      "Momentum: 0.4, Accuracy: 93.01%\n",
      "Momentum: 0.6, Accuracy: 94.04%\n",
      "Momentum: 0.8, Accuracy: 96.14%\n",
      "Momentum: 0.9, Accuracy: 97.45%\n",
      "Best momentum value: 0.9 with accuracy: 97.45%\n"
     ]
    }
   ],
   "source": [
    "# Test different momentum values\n",
    "momentum_values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "results = {}\n",
    "\n",
    "for momentum in momentum_values:\n",
    "    accuracy = train_and_evaluate(momentum)\n",
    "    results[momentum] = accuracy\n",
    "    print(f\"Momentum: {momentum}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Find the best momentum value\n",
    "best_momentum = max(results, key=results.get)\n",
    "print(f\"Best momentum value: {best_momentum} with accuracy: {results[best_momentum]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What happens if momentum is too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If momentum is too high, the optimizer might move too quickly and overshoot the best weights, causing the loss to fluctuate and making the training unstable. This can prevent the model from converging properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
