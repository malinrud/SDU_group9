{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tiANUo8y4LQY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3wN7Hvy4f0x",
        "outputId": "826d682c-7f81-401d-f2fd-a75b8bc2d66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4359007.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 128688.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:03<00:00, 525950.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3114358.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H5FHi3UY4kMF"
      },
      "outputs": [],
      "source": [
        "# 2. Define the neural network architecture\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        # 1x1 convolution branch\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        # 3x3 convolution branch (followed by a 1x1 convolution)\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # 5x5 convolution branch (followed by a 1x1 convolution)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        # 3x3 max pooling branch (followed by a 1x1 convolution)\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        branch_pool = self.branch_pool(x)\n",
        "\n",
        "        # Concatenating along the channel dimension\n",
        "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "\n",
        "class InceptionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InceptionNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.inception = InceptionModule(in_channels=64)\n",
        "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(128 * 7 * 7, 10)  # Input image size is 28x28 (MNIST)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inception(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.adaptive_avg_pool2d(x, (7, 7))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T3lNiah4sUz",
        "outputId": "e41a6f72-6172-4e14-cce6-27ec63733685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InceptionNet(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (inception): InceptionModule(\n",
            "    (branch1x1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (branch3x3): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (branch5x5): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "    (branch_pool): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# 3. Initialize the network, loss function, and optimizer\n",
        "model = InceptionNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BGN7e_Xv4t7I"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        output = model(data)   # Forward pass\n",
        "        loss = criterion(output, target)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N5GRfcR74vav"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjjWyAfx4wPu",
        "outputId": "7cf65a5d-2be2-45ed-893a-9f6471c9b902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303787\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.203742\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.032882\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.180824\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.022528\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.126758\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.073993\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.257571\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.083299\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.142185\n",
            "\n",
            "Test set: Average loss: 0.0010, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.141677\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.047995\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.089488\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.019702\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.007767\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.030970\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.056256\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.013558\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.047971\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.010550\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.018149\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.004716\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.012947\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.043219\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.157160\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.036963\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.005166\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.031487\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.040999\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.002407\n",
            "\n",
            "Test set: Average loss: 0.0007, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005427\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.016199\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.008958\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.015079\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003768\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.010783\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.022855\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007433\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001827\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.004351\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9893/10000 (99%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(1, 5):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(model, device, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cl_5oLmm4xWA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}